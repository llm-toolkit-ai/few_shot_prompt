
import os
from dotenv import load_dotenv
import requests
import json
load_dotenv()

def openai_few_shot_prompt(api_key, prompt, examples, model="gpt-4", temperature=0.7, max_tokens=100, stop=None):
    """
    Generates a response based on a few-shot prompt using the OpenAI API.

    Parameters:
    api_key (str): The API key for accessing the OpenAI API.
    prompt (str): The main prompt to generate a response from.
    examples (list of dict): A list of example prompts and responses to guide the model.
    model (str): The model to use for text generation (default is "gpt-4").
    temperature (float): Sampling temperature to control the creativity of the model (default is 0.7).
    max_tokens (int): The maximum number of tokens in the generated response (default is 100).
    stop (str or list): Optional stop sequence to end the generation.

    Returns:
    str: Response generated by the OpenAI API.
    """
    # Build the few-shot prompt
    few_shot_prompt = ""
    for example in examples:
        few_shot_prompt += f"Example prompt: {example['prompt']}\n"
        few_shot_prompt += f"Example response: {example['response']}\n\n"
    few_shot_prompt += f"Prompt: {prompt}\nResponse:"

    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {api_key}"
    }

    data = {
        "model": model,
        "messages": [
            {"role": "user", "content": few_shot_prompt}
        ],
        "temperature": temperature,
        "max_tokens": max_tokens,
        "stop": stop
    }

    response = requests.post("https://api.openai.com/v1/chat/completions", headers=headers, data=json.dumps(data))

    if response.status_code == 200:
        response_json = response.json()
        generated_response = response_json["choices"][0]["message"]["content"].strip()
        return generated_response
    else:
        return f"Error {response.status_code}: {response.text}"




def anthropic_few_shot_prompt(api_key, prompt, examples, model="claude-3-5-sonnet-20240620", temperature=0.7, max_tokens=1024):
    """
    Generates a response based on a few-shot prompt using the Anthropic API.

    Parameters:
    api_key (str): The API key for accessing the Anthropic API.
    prompt (str): The main prompt to generate a response from.
    examples (list of dict): A list of example prompts and responses to guide the model.
    model (str): The model to use for text generation (default is "claude-3-5-sonnet-20240620").
    temperature (float): Sampling temperature to control the creativity of the model (default is 0.7).
    max_tokens (int): The maximum number of tokens in the generated response (default is 1024).

    Returns:
    str: Response generated by the Anthropic API.
    """
    # Build the few-shot prompt
    few_shot_prompt = ""
    for example in examples:
        few_shot_prompt += f"Example prompt: {example['prompt']}\n"
        few_shot_prompt += f"Example response: {example['response']}\n\n"
    few_shot_prompt += f"Prompt: {prompt}\nResponse:"

    url = "https://api.anthropic.com/v1/messages"
    
    headers = {
        "x-api-key": api_key,
        "anthropic-version": "2023-06-01",
        "content-type": "application/json"
    }

    data = {
        "model": model,
        "max_tokens": max_tokens,
        "temperature": temperature,
        "messages": [
            {"role": "user", "content": few_shot_prompt}
        ]
    }

    response = requests.post(url, headers=headers, data=json.dumps(data))

    if response.status_code == 200:
        response_json = response.json()
        generated_response = response_json["content"][0]["text"].strip()
        return generated_response
    else:
        return f"Error {response.status_code}: {response.text}"


def run_mistral(api_key, user_message, model="mistral-medium-latest"):
    url = "https://api.mistral.ai/v1/chat/completions"
    
    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {api_key}"
    }

    data = {
        "model": model,
        "messages": [
            {"role": "user", "content": user_message}
        ],
        "temperature": 0.7,
        "top_p": 1.0,
        "max_tokens": 512,
        "stream": False,
        "safe_prompt": False,
        "random_seed": 1337
    }

    response = requests.post(url, headers=headers, data=json.dumps(data))

    if response.status_code == 200:
        response_json = response.json()
        return response_json["choices"][0]["message"]["content"].strip()
    else:
        return f"Error {response.status_code}: {response.text}"

def mistral_few_shot_prompt(api_key, prompt, examples, model="mistral-medium-latest", temperature=0.7, max_tokens=512):
    """
    Generates a response based on a few-shot prompt using the Mistral API.

    Parameters:
    api_key (str): The API key for accessing the Mistral API.
    prompt (str): The main prompt to generate a response from.
    examples (list of dict): A list of example prompts and responses to guide the model.
    model (str): The model to use for text generation (default is "mistral-medium-latest").
    temperature (float): Sampling temperature to control the creativity of the model (default is 0.7).
    max_tokens (int): The maximum number of tokens in the generated response (default is 512).

    Returns:
    str: Response generated by the Mistral API.
    """
    # Build the few-shot prompt
    few_shot_prompt = ""
    for example in examples:
        few_shot_prompt += f"Example prompt: {example['prompt']}\n"
        few_shot_prompt += f"Example response: {example['response']}\n\n"
    few_shot_prompt += f"Prompt: {prompt}\nResponse:"

    return run_mistral(api_key, few_shot_prompt, model=model)
